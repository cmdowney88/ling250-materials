{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lecture 6: NLTK Corpora\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is a powerful Python library for working with human language data. This tutorial covers accessing and exploring corpora, frequency distributions, and text analysis.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python and the `ling250` environment\n",
    "- NLTK installed (`pip install nltk` if needed)\n",
    "- `Night_Vale.txt` in your `data/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## What is NLTK?\n",
    "\n",
    "NLTK is an **open-source** Python library (free to use and modify) with a huge range of functionality:\n",
    "- Accessing, exploring, and manipulating corpora\n",
    "- Finding and visualizing statistical patterns in text\n",
    "- Tagging/annotating text for linguistic structure\n",
    "- Tokenization and basic NLP models\n",
    "\n",
    "It's excellent for **exploratory analysis** of text. We could easily spend weeks on just NLTK — today we'll focus on `nltk.corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Part 1: Accessing NLTK Corpora\n",
    "\n",
    "NLTK comes with many built-in corpora. Let's start with the **Gutenberg corpus** — a collection of public domain literature from [Project Gutenberg](https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Viewing available files\n",
    "\n",
    "Every corpus has a `.fileids()` method to see what files it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what files are in the Gutenberg corpus\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "**Note:** If you get an error saying the corpus isn't found, you may need to download it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if needed:\n",
    "# nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Levels of granularity\n",
    "\n",
    "We can access corpus text at different levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .raw() - raw characters (unprocessed)\n",
    "raw_text = gutenberg.raw('austen-emma.txt')\n",
    "print(type(raw_text))\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .words() - tokenized words (no sentence structure)\n",
    "words = gutenberg.words('austen-emma.txt')\n",
    "print(type(words))\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .sents() - sentences (list of lists)\n",
    "sents = gutenberg.sents('austen-emma.txt')\n",
    "print(type(sents))\n",
    "print(sents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**Note:** If `.sents()` gives an error, you may need to download the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if needed:\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Part 2: The Brown Corpus\n",
    "\n",
    "The Brown Corpus is a collection of American English texts from different genres. What makes it special is that texts are **categorized** by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# What genres are available?\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Accessing specific categories\n",
    "\n",
    "We can filter by category when accessing words or sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words from only the 'news' category\n",
    "news_words = brown.words(categories='news')\n",
    "print(news_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple categories\n",
    "fiction_words = brown.words(categories=['science_fiction', 'romance'])\n",
    "print(len(fiction_words), \"words in sci-fi and romance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Part 3: Loading Your Own Text Files\n",
    "\n",
    "You can treat your own text files as a corpus using `PlaintextCorpusReader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Create a corpus from files in the ../data/ directory\n",
    "# First argument: directory path\n",
    "# Second argument: file pattern(s)\n",
    "my_corpus = PlaintextCorpusReader('../data/', 'Night_Vale.txt')\n",
    "\n",
    "# Now we can use the same methods\n",
    "print(my_corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words from our Night Vale corpus\n",
    "nv_words = my_corpus.words('Night_Vale.txt')\n",
    "print(len(nv_words), \"words\")\n",
    "print(nv_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "**Tip:** You can use wildcards to load multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all .txt files in a directory\n",
    "# multi_corpus = PlaintextCorpusReader('../data/', '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Part 4: Frequency Distributions\n",
    "\n",
    "A `FreqDist` counts occurrences in a list of items (usually words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Count word frequencies in Emma\n",
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "fdist = FreqDist(emma_words)\n",
    "\n",
    "print(type(fdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Useful FreqDist methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single most common word\n",
    "fdist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times does a specific word appear?\n",
    "fdist['Emma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate top items\n",
    "fdist.tabulate(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequencies\n",
    "fdist.plot(30)\n",
    "\n",
    "# If the plot doesn't show, you may need:\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Night Vale example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in Night Vale\n",
    "nv_fdist = FreqDist(nv_words)\n",
    "nv_fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "**Observation:** Most common words are function words (\"the\", \"a\", \"to\", etc.). To find interesting content words, we might want to filter these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out short words and punctuation\n",
    "content_words = [w for w in nv_words if len(w) > 3 and w.isalpha()]\n",
    "content_fdist = FreqDist(content_words)\n",
    "content_fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Part 5: Conditional Frequency Distributions\n",
    "\n",
    "A `ConditionalFreqDist` counts occurrences based on **conditions**. It takes a list of **pairs** where:\n",
    "- First item = condition (e.g., genre, category)\n",
    "- Second item = sample (e.g., word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "\n",
    "# Example: Count words by genre in Brown corpus\n",
    "# Create pairs of (genre, word)\n",
    "genre_word_pairs = [\n",
    "    (genre, word)\n",
    "    for genre in ['news', 'romance']\n",
    "    for word in brown.words(categories=genre)\n",
    "]\n",
    "\n",
    "print(genre_word_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ConditionalFreqDist\n",
    "cfd = ConditionalFreqDist(genre_word_pairs)\n",
    "print(type(cfd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Useful ConditionalFreqDist methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What conditions (genres) do we have?\n",
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific condition's FreqDist\n",
    "cfd['news'].most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare specific words across conditions\n",
    "# Use samples parameter to specify which words to show\n",
    "cfd.tabulate(samples=['the', 'she', 'he', 'love', 'said', 'news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison (limit to specific words)\n",
    "cfd.plot(samples=['she', 'he', 'love', 'said', 'news', 'man', 'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### More genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare more genres\n",
    "genres = ['news', 'romance', 'science_fiction', 'humor']\n",
    "genre_word_pairs = [\n",
    "    (genre, word.lower())  # lowercase for case-insensitive comparison\n",
    "    for genre in genres\n",
    "    for word in brown.words(categories=genre)\n",
    "]\n",
    "\n",
    "cfd_multi = ConditionalFreqDist(genre_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare interesting words\n",
    "cfd_multi.tabulate(samples=['love', 'space', 'time', 'funny', 'said', 'president'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## Part 6: Text Objects\n",
    "\n",
    "NLTK's `Text` class wraps a list of words and provides convenient analysis methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "# Create a Text object from our Night Vale words\n",
    "nv_text = Text(nv_words)\n",
    "print(type(nv_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "**Important:** `Text()` expects a **list of words**, not a list of sentences. If you pass it sentences, it will treat each sentence as a single \"word\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Useful Text methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance: see a word in context\n",
    "nv_text.concordance('desert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar words: words appearing in similar contexts\n",
    "nv_text.similar('desert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common contexts: where do these words appear?\n",
    "nv_text.common_contexts(['desert', 'city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences\n",
    "nv_text.count('Cecil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocations: common two-word phrases\n",
    "nv_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispersion plot: where do words appear in the text?\n",
    "nv_text.dispersion_plot(['Cecil', 'Carlos', 'desert', 'radio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Emma as a Text object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_text = Text(emma_words)\n",
    "emma_text.concordance('Emma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Part 7: Bigrams\n",
    "\n",
    "A **bigram** is a pair of consecutive words. Bigrams help us understand which words tend to follow other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "\n",
    "# Simple example\n",
    "text = ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "bi = bigrams(text)\n",
    "\n",
    "# bigrams() returns a generator, so convert to list to see it\n",
    "print(list(bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "**Note on generators:** A generator is like a lazy list — it produces items one at a time rather than storing them all in memory. Converting to a list forces it to generate everything at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a second use, create the generator again\n",
    "bi = bigrams(text)\n",
    "print(list(bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Bigrams with real text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bigrams from Emma\n",
    "emma_bigrams = list(bigrams(emma_words))\n",
    "print(len(emma_bigrams), \"bigrams\")\n",
    "print(emma_bigrams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### Bigrams with ConditionalFreqDist\n",
    "\n",
    "Since bigrams are **pairs**, we can use them with `ConditionalFreqDist`:\n",
    "- Condition = first word\n",
    "- Sample = second word\n",
    "\n",
    "This tells us: \"Given the first word, what words commonly follow?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CFD from bigrams\n",
    "emma_cfd = ConditionalFreqDist(emma_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What words follow \"Mr\"?\n",
    "emma_cfd['Mr'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What words follow \"she\"?\n",
    "emma_cfd['she'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Text generation with bigrams\n",
    "\n",
    "We can use bigram frequencies to generate text — always pick the most common following word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive generation: always pick .max()\n",
    "current_word = 'Emma'\n",
    "generated = [current_word]\n",
    "\n",
    "for i in range(20):\n",
    "    # Get most common word that follows current_word\n",
    "    next_word = emma_cfd[current_word].max()\n",
    "    generated.append(next_word)\n",
    "    current_word = next_word\n",
    "\n",
    "print(' '.join(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "**Problem:** This often gets stuck in loops! Always choosing the *most* common word is too deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Better generation: random sampling\n",
    "\n",
    "Instead of always picking the most common word, we can randomly sample based on frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "current_word = 'Emma'\n",
    "generated = [current_word]\n",
    "\n",
    "for i in range(30):\n",
    "    # Get all possible next words and their frequencies\n",
    "    following_words = list(emma_cfd[current_word].keys())\n",
    "    frequencies = list(emma_cfd[current_word].values())\n",
    "    \n",
    "    # If no following words, stop\n",
    "    if not following_words:\n",
    "        break\n",
    "    \n",
    "    # Randomly choose weighted by frequency\n",
    "    next_word = random.choices(following_words, weights=frequencies)[0]\n",
    "    generated.append(next_word)\n",
    "    current_word = next_word\n",
    "\n",
    "print(' '.join(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running the above cell multiple times - you get different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## Part 8: Lexical Resources\n",
    "\n",
    "NLTK includes several lexical resources. One of the most powerful is **WordNet**, a large lexical database of English.\n",
    "\n",
    "WordNet groups words into **synsets** (synonym sets) and provides:\n",
    "- Definitions\n",
    "- Examples\n",
    "- Relationships between words (synonyms, antonyms, hypernyms, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Uncomment if needed:\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get synsets for a word\n",
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get definition of first synset\n",
    "dog = wn.synset('dog.n.01')\n",
    "print(dog.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples\n",
    "dog.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hypernyms (more general terms)\n",
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyponyms (more specific terms)\n",
    "dog.hyponyms()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "**Note:** We'll explore WordNet more in future lectures. For now, just know it exists as a powerful resource for understanding word relationships!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "Try these on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### Exercise 1: Most common words in different genres\n",
    "\n",
    "Compare the top 20 most common words (excluding punctuation and short words) in the 'news' and 'humor' categories of the Brown corpus. What differences do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### Exercise 2: Character names in Night Vale\n",
    "\n",
    "Find the most common capitalized words in Night Vale (hint: use regex or check if `word[0].isupper()`). These are likely to include character names and locations. What are the top 20?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### Exercise 3: Comparing corpora with Text objects\n",
    "\n",
    "Create Text objects for both Night Vale and one of the Gutenberg texts. Use `.similar()` to compare which words appear in similar contexts to \"night\" in each text. What differences do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### Exercise 4: Generate from Night Vale\n",
    "\n",
    "Create a bigram-based text generator for Night Vale. Start with the word \"Welcome\" and generate 50 words. Try running it several times. Does it capture any Night Vale flavor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Accessing Corpora\n",
    "\n",
    "| Method | Returns |\n",
    "|--------|--------|\n",
    "| `.fileids()` | List of file names |\n",
    "| `.raw(fileid)` | Raw text as string |\n",
    "| `.words(fileid)` | List of words |\n",
    "| `.sents(fileid)` | List of sentences (each sentence is a list of words) |\n",
    "| `.categories()` | List of categories (if applicable) |\n",
    "\n",
    "### FreqDist\n",
    "\n",
    "| Method | Returns |\n",
    "|--------|--------|\n",
    "| `.most_common(n)` | List of (word, count) tuples for top n words |\n",
    "| `.max()` | Most common word |\n",
    "| `[word]` | Count of specific word |\n",
    "| `.tabulate(n)` | Print table of top n words |\n",
    "| `.plot(n)` | Plot frequency graph |\n",
    "\n",
    "### ConditionalFreqDist\n",
    "\n",
    "| Method | Returns |\n",
    "|--------|--------|\n",
    "| `.conditions()` | List of conditions |\n",
    "| `[condition]` | FreqDist for that condition |\n",
    "| `.tabulate(samples=[...])` | Print table comparing conditions |\n",
    "| `.plot(samples=[...])` | Plot comparison |\n",
    "\n",
    "### Text Objects\n",
    "\n",
    "| Method | Does |\n",
    "|--------|-----|\n",
    "| `.concordance(word)` | Show word in context |\n",
    "| `.similar(word)` | Find words in similar contexts |\n",
    "| `.common_contexts([words])` | Find shared contexts |\n",
    "| `.count(word)` | Count occurrences |\n",
    "| `.collocations()` | Find common phrases |\n",
    "| `.dispersion_plot([words])` | Plot word positions |\n",
    "\n",
    "### Other\n",
    "\n",
    "| Function | Does |\n",
    "|----------|-----|\n",
    "| `nltk.download('name')` | Download NLTK data |\n",
    "| `bigrams(words)` | Create bigrams from word list |\n",
    "| `PlaintextCorpusReader(dir, files)` | Create corpus from local files |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [NLTK Book Chapter 1](https://www.nltk.org/book/ch01.html) — Language Processing and Python\n",
    "- [NLTK Book Chapter 2](https://www.nltk.org/book/ch02.html) — Accessing Text Corpora and Lexical Resources\n",
    "- [NLTK Corpus HOWTO](https://www.nltk.org/howto/corpus.html)\n",
    "- [NLTK Documentation](https://www.nltk.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ling250",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
